#--------------------------------------------------------------------------------
# This Python script is designed to train a neural network model using the MNIST dataset on an AWS Trainium 32x large instance,
  # utilizing its computational power. The script defined in Dickerfile imports essential libraries, including Torch, and defines constants for the number of epochs, warm-up steps, and batch size.
  # It then loads the MNIST training dataset, initializes the model architecture (MLP), the optimizer, and the loss function.

# The training loop iterates over the specified number of epochs and batches.
  # For each batch, the script computes the model's output, calculates the loss,
  # performs backpropagation, and optimizes the model's parameters using the defined optimizer.
  # During training, statistics like training throughput and loss are computed and printed.

# After training, the script saves a checkpoint of the model's state for evaluation.
  # It creates a directory for checkpoints and stores the model's state dictionary in a checkpoint file.

# In summary, this script demonstrates how to utilize the computational capabilities of an
  # AWS Trainium 32x large instance to efficiently train a machine learning model on the MNIST dataset.

# MNIST Dataset:
# The MNIST dataset is a widely used benchmark dataset in the field of machine learning and computer vision.
#   It consists of a collection of handwritten digit images, each labeled with the corresponding digit (0 to 9).
#   The dataset contains 60,000 training images and 10,000 test images, making it a standard dataset for evaluating image classification algorithms.
#   MNIST is often used to develop and test algorithms for digit recognition, image processing, and machine learning model training.

# Multi-Layer Perceptron (MLP):
# An MLP is a type of artificial neural network that consists of multiple layers of interconnected nodes, also known as neurons.
#   It is a feedforward neural network architecture, meaning the data flows in one direction from input to output through the hidden layers.
#   Each neuron in one layer is connected to every neuron in the adjacent layers.
#--------------------------------------------------------------------------------

#--------------------------------------------------------------------------------
# Step 1: Execute the 1-build-image.sh shell script to create a new ECR repository, build the Docker image, and push it to ECR. Refer to the Dockerfile and the shell script for additional details.
# Step 2: Prior to running the command, make sure to perform the following updates in the snippet below:
#     Replace <ACCOUNT_ID> with your actual AWS account ID and <REGION> with the AWS region in which you intend to deploy the job.
#     Alternatively, you can update the entire Image URI with the appropriate tag.
# Step3: Apply(`kubectl apply -f trn1-mlp-karpenter.yaml`) the following YAML file.
#   It might be necessary to apply it twice since queue creation could take some time. Wait for the queue to be created before proceeding to job submission.
#--------------------------------------------------------------------------------

---
apiVersion: v1
kind: Namespace
metadata:
  name: ml-team-a-ns

# Volcano dedicated queue for ml-team-a
---
apiVersion: scheduling.volcano.sh/v1beta1
kind: Queue
metadata:
  name: ml-team-a-vq
spec:
  reclaimable: false
  weight: 1

---
apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: trn1-mlp-job
  namespace: ml-team-a-ns
spec:
  schedulerName: volcano
  queue: ml-team-a-vq
  tasks:
    - replicas: 1
      template:
        metadata:
          labels:
            WorkerType: trn1-32xl
        spec:
          restartPolicy: Never
          containers:
            - name: trn1-mlp
              command: ['torchrun']
              args:
                - '--nnodes=1'
                - '--nproc_per_node=32'
                - 'train_torchrun.py'
              image: <ACCOUNT_ID>.dkr.ecr.<REGION>.amazonaws.com/eks_mlperf_training:mlp
              imagePullPolicy: IfNotPresent
              resources:
                limits:
                  aws.amazon.com/neuron: 16
                  cpu: 2
                  # vpc.amazonaws.com/efa: 8. # NOTE: This is not supported by Karpenter v0.29.0  yet so dont request EFA
          nodeSelector:
            provisioner: karpenter
            instance-type: trn1-32xl
          tolerations:
            - key: "aws.amazon.com/neuron"
              value: "true"
              effect: "NoSchedule"

# Use the below script without Volcano scheduler and Karpenter provisioner
# ---
# apiVersion: v1
# kind: Pod
# metadata:
#   name: trn1-mlp
# spec:
#   restartPolicy: Never
#   schedulerName: default-scheduler
#   nodeSelector:
#     # provisioner: cluster-autoscaler # To be used with Managed nodegroups with Cluster Autoscaler
#     provisioner: karpenter
#     instance-type: trn1-32xl
#   tolerations:
#     - key: "aws.amazon.com/neuron"
#       value: "true"
#       effect: "NoSchedule"
#   containers:
#     - name: trn1-mlp
#       command: ['torchrun']
#       args:
#         - '--nnodes=1'
#         - '--nproc_per_node=32'
#         - 'train_torchrun.py'
#       image: <ACCOUNTID>.dkr.ecr.<REGION>.amazonaws.com/eks_mlperf_training:mlp
#       imagePullPolicy: IfNotPresent
#       resources:
#         limits:
#           aws.amazon.com/neuron: 16
